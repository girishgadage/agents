{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read the functionality</h2>\n",
    "            <span style=\"color:#ff7800;\">This code asks for User request and then asks 4 LLMS (openAi, Gemini, Groq and Ollama) for a response for the user request </br> Then it evalutes each of the response with the 4 LLMS and grades them and finally displays the grading as a HTML Table\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import asyncio\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, HTML, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get multiline input from the user as the request\n",
    "\n",
    "def multiline_input_widget_auto(\n",
    "    var_name=\"user_text\",\n",
    "    title=\"Enter your challenging request here:\",\n",
    "    placeholder=\"Type your request here…\",\n",
    "    button_text=\"Submit\",\n",
    "    height=\"200px\",\n",
    "    width=\"100%\"\n",
    "):\n",
    "    ta = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=placeholder,\n",
    "        layout=widgets.Layout(width=width, height=height)\n",
    "    )\n",
    "    btn = widgets.Button(description=button_text)\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def on_click(_):\n",
    "        globals()[var_name] = ta.value  # Save to a Python variable\n",
    "        with out:\n",
    "            clear_output()\n",
    "            print(f\"✅ Saved to variable '{var_name}'\")\n",
    "\n",
    "    btn.on_click(on_click)\n",
    "    display(widgets.VBox([widgets.HTML(f\"<b>{title}</b>\"), ta, btn, out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user request\"\n",
    "multiline_input_widget_auto(\"user_request\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay the user_request\n",
    "print (\"Your request entered is :\")\n",
    "print (user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from the user as to the challenging question he would like to have an best answer for\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Class to store models and its answers\n",
    "class ModelResponseStorage:\n",
    "    def __init__(self):\n",
    "        # Internal storage for all pairs\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, ai_model, response):\n",
    "        \"\"\"Add a pair of text values to storage.\"\"\"\n",
    "        self.data.append([ai_model, response])\n",
    "        #print(f\"✅ Added: [{text1}, {text2}]\")\n",
    "\n",
    "    def get_all(self):\n",
    "        \"\"\"Return all stored pairs as a list.\"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def get_first_values(self):\n",
    "        \"\"\"Return a list of all first values in the pairs.\"\"\"\n",
    "        return [pair[0] for pair in self.data]\n",
    "\n",
    "    def get_second_values(self):\n",
    "        \"\"\"Return a list of all second values in the pairs.\"\"\"\n",
    "        return [pair[1] for pair in self.data]\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Pretty print all stored pairs with an index.\"\"\"\n",
    "        for idx, (first, second) in enumerate(self.data, start=1):\n",
    "            print(f\"{idx}: {first} — {second}\")\n",
    "\n",
    "    def unzip(self):\n",
    "        \"\"\"Return two separate lists: first values, second values.\"\"\"\n",
    "        if not self.data:\n",
    "            return [], []\n",
    "        first_values, second_values = zip(*self.data)\n",
    "        return list(first_values), list(second_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the class to store the values\n",
    "store_response= ModelResponseStorage ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the User request to the OpenAI and store the response for evaluation\n",
    "ai_model=\"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=ai_model,\n",
    "    messages=messages,\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "#Store the model and its answer in our array\n",
    "store_response.add(ai_model,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the User request to the Gemini and store the response for evaluation\n",
    "ai_model = \"gemini-2.0-flash\"\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "response = gemini.chat.completions.create(model=ai_model, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "#Store the model and its answer in our array\n",
    "store_response.add(ai_model,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the User request to the Groq and store the response for evaluation\n",
    "\n",
    "ai_model = \"llama-3.3-70b-versatile\"\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "\n",
    "response = groq.chat.completions.create(model=ai_model, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "#Store the model and its answer in our array\n",
    "store_response.add(ai_model,answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the User request to the ollama and store the response for evaluation\n",
    "\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "ai_model = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=ai_model, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "#Store the model and its answer in our array\n",
    "store_response.add(ai_model,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the information from the Class Instance\n",
    "\n",
    "competing_models = store_response.get_first_values()\n",
    "model_answers = store_response.get_second_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(model_answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a best response given by {len(competing_models)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{user_request}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"Rank1\", \"Rank2\", \"Rank3\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "#using OpenAI model gpt-4o-mini\n",
    "eval_model=\"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model= eval_model,\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "#print(ranks)\n",
    "row1string = eval_model \n",
    "#print(\"Juding Using Model:\"+ eval_model)\n",
    "for index, result in enumerate(ranks):\n",
    "    number = int(result[-1])-1\n",
    "    competitor = competing_models[number]\n",
    "    answer = model_answers[number]\n",
    "    row1string += \",\" + competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "#using Gemini model gemini-2.0-flash\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "eval_model = \"gemini-2.0-flash\"\n",
    "response = gemini.chat.completions.create(\n",
    "    model=eval_model,\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "#print(ranks)\n",
    "row2string = eval_model \n",
    "#print(\"Juding Using Model:\"+ eval_model)\n",
    "for index, result in enumerate(ranks):\n",
    "    number = int(result[-1])-1\n",
    "    #print(number)\n",
    "    competitor = competing_models[number]\n",
    "    #print(competitor)\n",
    "    answer = model_answers[number]\n",
    "    row2string += \",\" + competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "#using Groq\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "eval_model = \"llama-3.3-70b-versatile\"\n",
    "response = groq.chat.completions.create(\n",
    "    model=eval_model,\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "#print(ranks)\n",
    "row3string = eval_model \n",
    "#print(\"Juding Using Model:\"+ eval_model)\n",
    "for index, result in enumerate(ranks):\n",
    "    number = int(result[-1])-1\n",
    "    #print(number)\n",
    "    competitor = competing_models[number]\n",
    "    #print(competitor)\n",
    "    answer = model_answers[number]\n",
    "    row3string += \",\" + competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "#using ollama\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "eval_model = \"llama3.2\"\n",
    "response = ollama.chat.completions.create(\n",
    "    model=eval_model,\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "#print(ranks)\n",
    "row4string = eval_model \n",
    "#print(\"Juding Using Model:\"+ eval_model)\n",
    "for index, result in enumerate(ranks):\n",
    "    number = int(result[-1])-1\n",
    "    competitor = competing_models[number]\n",
    "    answer = model_answers[number]\n",
    "    row4string += \",\" + competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeaderRow=\"Eval_Model\" + \",\" +  \"Rank1\"+ \",\" + \"Rank2\"+ \",\" + \"Rank3\"+ \",\" + \"Rank4\"\n",
    "#print(HeaderRow)\n",
    "#print(row1string)\n",
    "#print(row2string)\n",
    "#print(row3string)\n",
    "#print(row4string)\n",
    "\n",
    "#Build a table using Pandas Table\n",
    "rows = [HeaderRow, row1string, row2string, row3string, row4string]\n",
    "\n",
    "# Split each row into a list of values\n",
    "data = [row.split(\",\") for row in rows]\n",
    "\n",
    "# Create a DataFrame: first row = column names, rest = data\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "#display table as html in this book\n",
    "display(HTML(df.to_html(index=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
